{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TODO</h3> \n",
    "Fill in any place that says `YOUR CODE HERE`.\n",
    "\n",
    "<h3>Suggestions</h3>\n",
    "\n",
    "- To speed up your code, think about how certain operations can be done at the same time.\n",
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "- Double check your code does not have $\\infty$-loops, these will crash the autograder.\n",
    "\n",
    "<h3>Rules</h3>\n",
    "\n",
    "- Blank cells in the notebook are hidden tests. **Do not delete, copy, paste, or alter these cells as this will cause the tests to fail automatically**.\n",
    "- Do not create multiple python notebooks (.ipynb files).\n",
    "- Do not import any new python packages (this may cause hidden tests to fail).\n",
    "- Each cell must run for less than 5 minutes (there exists a solution with full marks.\n",
    "- **Do not plagiarise!** We take violations of this very seriously. In previous years we have identified instances of plagiarism and reported them to the Senior Teaching & Learning Administrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "391ec8576123080cf705cb0c7f8ae9c0",
     "grade": false,
     "grade_id": "cell-4beed8a45acd7af1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import time\n",
    "from cvxpy import *\n",
    "from scipy.spatial.distance import cdist\n",
    "from visclassifier import visclassifier\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import pylab\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7215b06812a39c419fc22393b1b3fb37",
     "grade": false,
     "grade_id": "cell-5f452d4d6d57362d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "In this project, you will implement a linear support vector machine and one operating in kernel space. For this you will need to formulate the primal and dual optimization problems as quadratic programs. You will be using <code>cvxpy</code>. Before we get started please read through the <a href=\"https://www.cvxpy.org/tutorial/intro/index.html\" target=\"_blank\">tutorial</a> and look through some <a href=\"https://www.cvxpy.org/examples/\" target=\"_blank\">examples</a> of cvxpy, in particular its <a href=\"https://colab.research.google.com/github/cvxpy/cvxpy/blob/master/examples/notebooks/WWW/quadratic_program.ipynb\" target=\"_blank\">quadratic programming solver</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82e57c880a8ab41a60e1f364178e97eb",
     "grade": false,
     "grade_id": "cell-63c47f087db65e79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h4> Linear classification</h4>\n",
    "\n",
    "<p> The first assignment is to implement a linear support vector machine. Before we get started we can generate some data to see if everything is working:  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "631672af18a8ef3f92e4a4280128eab3",
     "grade": false,
     "grade_id": "cell-2b19f0dd3cb286da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def genrandomdata(n=100,b=0.):\n",
    "    # generate random data and linearly-separable labels\n",
    "    xTr = np.random.randn(n, 2)\n",
    "    # defining random hyperplane\n",
    "    w0 = np.random.rand(2, 1)\n",
    "    # assigning labels +1, -1 labels depending on what side of the plane they lie on\n",
    "    yTr = np.sign(np.dot(xTr, w0)+b).flatten()\n",
    "    return xTr, yTr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "219f6726a68ceb3429b8ef2ff514a583",
     "grade": false,
     "grade_id": "cell-f8347f448bc9596b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>Remember the SVM primal formulation\n",
    "$$\\begin{aligned}\n",
    "             &\\min_{\\mathbf{w},b,\\xi} \\|\\mathbf{w}\\|^2_2+C \\sum_{i=1}^n \\xi_i\\\\\n",
    "       & \\text{such that }  \\ \\forall i:\\\\\n",
    "             & y_i(\\mathbf{w}^\\top \\mathbf{x}_i+b)\\geq 1-\\xi_i\\\\\n",
    "             & \\xi_i\\geq 0.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "You will need to implement  the function <code>primalSVM</code>, which takes in training data <code>xTr</code> ($n\\times d$) and labels <code>yTr</code> ($n$) with <code>yTr[i]</code>$\\in \\{-1,1\\}$. Currently, the code below is a placeholder example of a <code>cvxpy</code> optimization problem. You need to update the objective, the constraints and introduce new variables to output the correct hyperplane and bias. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c42535e225170c36b26d739734b5d15a",
     "grade": false,
     "grade_id": "cell-767cf779d2d4b68e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def primalSVM(xTr, yTr, C=1):\n",
    "    \"\"\"\n",
    "    function (classifier,w,b) = primalSVM(xTr,yTr;C=1)\n",
    "    constructs the SVM primal formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        xTr   | training data (nxd)\n",
    "        yTr   | training labels (n)\n",
    "        C     | the SVM regularization parameter\n",
    "    \n",
    "    Output:\n",
    "        fun   | a prediction function, usage: predictions=fun(xTe); where predictions.shape = (n,)\n",
    "        wout  | the weight vector calculated by the solver\n",
    "        bout  | the bias term calculated by the solver\n",
    "    \"\"\"\n",
    "    N, d = xTr.shape\n",
    "    y = yTr.flatten()\n",
    "    # example code: an example of establishing objective and constraints, and how to let the solver solve it.\n",
    "    w = Variable(d)\n",
    "    b = Variable(1)\n",
    "    objective = sum_squares(w)\n",
    "    constraints = [w >= 0]\n",
    "    prob = Problem(Minimize(objective), constraints)\n",
    "    prob.solve()\n",
    "    wout = w.value\n",
    "    bout = b.value\n",
    "    # End of example code\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    fun = lambda x: x.dot(wout) + bout\n",
    "    return fun, wout, bout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f6a08a5830307ee949585f33435287e",
     "grade": false,
     "grade_id": "cell-54f91155ad52bf49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can test your SVM primal solver with the following randomly generated data set. We label it in a way that it is guaranteed to be linearly separable. If your code works correctly the hyper-plane should separate all the $x$'s into the red half and all the $o$'s into the blue half. With sufficiently large values of $C$ (e.g. $C>10$) you should obtain $0\\%$ training error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aed611be3de4942bcc1686fbc16f5d90",
     "grade": false,
     "grade_id": "cell-f081e2455c2299ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def arrayify(x):\n",
    "    \"\"\"flattens and converts to numpy\"\"\"\n",
    "    return np.array(x).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6f8f38e28225970a733e657abf7d451",
     "grade": false,
     "grade_id": "cell-5178de8375ab9209",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xTr,yTr=genrandomdata()\n",
    "fun,w,b=primalSVM(xTr,yTr,C=10)\n",
    "visclassifier(fun,xTr,yTr,w=w,b=b)\n",
    "\n",
    "\n",
    "err=np.mean(arrayify(np.sign(fun(xTr)))!=yTr)\n",
    "print(\"Training error: %2.1f%%\" % (err*100))\n",
    "# Training error should be ~4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7950cbf83844d740dda3200dadedc9b3",
     "grade": false,
     "grade_id": "cell-03ea9de08009da46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Example Test</h3> (use this as a starting point for making new tests!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testCase_Primal():\n",
    "    Clocal = 1\n",
    "    trainX = np.array([[0,0],[0,1],[0,2],[1,0],[1,1],[1,2],[2,0],[2,1],[2,2]]) \n",
    "    trainY = np.array([1,1,1,-1,1,1,-1,-1,1])\n",
    "    fun_Primal,_,_ = primalSVM(trainX,trainY,C=Clocal)\n",
    "\n",
    "    testX = np.array([[1.6,0.4],[1.4,1.6],[0.4,0.6],[0.4,1.6]])\n",
    "    testY = np.array([-1,1,1,1])\n",
    "    resultY = fun_Primal(testX)\n",
    "    boolArray = (np.sign(resultY)==testY).tolist()\n",
    "    signMatch = all(boolArray)\n",
    "    return signMatch\n",
    "print('primalSVM passes sign match test: ' + str(testCase_Primal()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10107dec6a15349b4afe0c31a597e88a",
     "grade": true,
     "grade_id": "cell-87d9677909636fa7",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 1: testCase_Primal\n",
    "# ------------------------------\n",
    "# Given a fixed training set, this tests whether points farther from the decision boundary have larger predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fac448c8e1fb2e3a696c4fc8f3f7e54",
     "grade": false,
     "grade_id": "cell-4cf2a097ae2cdf69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Helper functions used to create animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02c515eb642e4ad88f5542d940d2409b",
     "grade": false,
     "grade_id": "cell-6470a47091c96546",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def updateboundary():\n",
    "    global w,b,Xdata,ldata,stepsize\n",
    "\n",
    "    _, w_pre, b_pre = primalSVM(np.transpose(Xdata),np.array(ldata),C=10)\n",
    "    w = np.array(w_pre).reshape(-1)\n",
    "    b = b_pre\n",
    "    stepsize+=1\n",
    "\n",
    "def updatescreen():\n",
    "    global w,b,ax,line \n",
    "    q=-b/(w**2).sum()*w;\n",
    "    if line==None:\n",
    "        line, = ax.plot([q[0]-w[1],q[0]+w[1]],[q[1]+w[0],q[1]-w[0]],'b--')\n",
    "    else:\n",
    "        line.set_ydata([q[1]+w[0],q[1]-w[0]])\n",
    "        line.set_xdata([q[0]-w[1],q[0]+w[1]])\n",
    "    \n",
    "def animate(i):\n",
    "    #if len(ldata)>0 and ((min(ldata)+max(ldata))==0):\n",
    "    if (len(ldata)>0) and ((np.min(ldata)+np.max(ldata))==0):\n",
    "        if stepsize<1000:\n",
    "            updateboundary()\n",
    "            updatescreen();\n",
    "    \n",
    "def onclick(event):\n",
    "    global Xdata, stepsize  \n",
    "    if event.key == 'shift': # add positive point\n",
    "        ax.plot(event.xdata,event.ydata,'or')\n",
    "        label=1\n",
    "    else: # add negative point\n",
    "        ax.plot(event.xdata,event.ydata,'ob')\n",
    "        label=-1    \n",
    "    pos=np.array([[event.xdata],[event.ydata]])\n",
    "    ldata.append(label);\n",
    "    Xdata=np.hstack((Xdata,pos))\n",
    "    stepsize=1;\n",
    "\n",
    "    #if (len(ldata)>0) and ((np.min(ldata)+np.max(ldata))==0):\n",
    "    #    if stepsize<1000:\n",
    "    #        updateboundary()\n",
    "    #        updatescreen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "121c2fecb61288190787c15f9e3decbc",
     "grade": false,
     "grade_id": "cell-d3d889b98c3e03e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46f50f43d78de647a6e4a000607e8042",
     "grade": false,
     "grade_id": "cell-7a1dd11f54729cc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Xdata=np.random.rand(2,0)\n",
    "ldata=[]\n",
    "w=[]\n",
    "b=[]\n",
    "line=None\n",
    "stepsize=1;\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "ani = FuncAnimation(fig, animate,np.arange(1,100,1),interval=10);\n",
    "plt.title('Use shift-click to add negative points.')\n",
    "# Note that it is easy to generate odd decision boundaries, particularly if data is not easily linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f85ff9f9468f6188a1868a9ed424c9f1",
     "grade": false,
     "grade_id": "cell-fc6595df8ebc2f14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Spiral data set</h3>\n",
    "\n",
    "<p>The linear classifier works great in simple linear cases. But what if the data is more complicated? We provide you with a \"spiral\" data set. You can load it and visualize it with the following two code snippets:\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "057a3db427fb3e72f839c72572044ec6",
     "grade": false,
     "grade_id": "cell-b707721c5c88ff46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N)\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    return xTr,yTr,xTe,yTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "337a6f2f0b294de468c5c4e344a2fe14",
     "grade": false,
     "grade_id": "cell-09fd0d13d248f578",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "fig = plt.figure()\n",
    "plt.scatter(xTr[yTr == 1, 0], xTr[yTr == 1, 1], c='b')\n",
    "plt.scatter(xTr[yTr != 1, 0], xTr[yTr != 1, 1], c='r')\n",
    "plt.legend([\"+1\",\"-1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e967cf48081d9f2bd3556ff80da6b81",
     "grade": false,
     "grade_id": "cell-0868475fb5c266a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>If you apply your previously functioning linear classifier on this data set you will see that you get terrible results. Your training error will increase drastically. Further, if you increase $C$ the convex solver will even refuse to optimize as the initial solution is not even feasible! </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3f939b402998885b9a805a6c20653b7",
     "grade": false,
     "grade_id": "cell-7d1ebe38c2b8d1c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fun,w,b=primalSVM(xTr,yTr,C=0.01)\n",
    "visclassifier(fun,xTr,yTr,w=[],b=0)\n",
    "err=np.mean(arrayify(np.sign(fun(xTr)))!=yTr)\n",
    "print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6887da7011d953b7c119b34d276f1b0",
     "grade": false,
     "grade_id": "cell-c7f79f4f5d5e55fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Implementing a kernelized SVM</h3>\n",
    "\n",
    "<p> For a data set as complex as the spiral data set, you will need a more complex classifier. \n",
    "First implement the kernel function\n",
    "<pre>\tcomputeK(kerneltype,X,Z,kpar)</pre>\n",
    "It takes as input a kernel type (kerneltype) and two data sets $\\mathbf{X}$ in $\\mathcal{R}^{n\\times d}$ and $\\mathbf{Z}$ in $\\mathcal{R}^{m\\times d}$ and outputs a kernel matrix $\\mathbf{K}\\in{\\mathcal{R}^{n\\times m}}$. The last input, <code>kpar</code> specifies the kernel parameter (e.g. the inverse kernel width $\\gamma$ in the RBF case or the degree $p$ in the polynomial case.)\n",
    "\t<ol>\n",
    "\t<li>For the linear kernel (<code>ktype='linear'</code>) svm, use $k(\\mathbf{x},\\mathbf{z})=x^Tz$ </li> \n",
    "\t<li>For the radial basis function kernel (<code>ktype='rbf'</code>) svm use $k(\\mathbf{x},\\mathbf{z})=\\exp(-\\gamma ||x-z||^2)$ (gamma is a hyperparameter, passed a the value of kpar)</li>\n",
    "\t<li>For the polynomial kernel (<code>ktype='poly'</code>) use  $k(\\mathbf{x},\\mathbf{z})=(x^Tz + 1)^d$ (d is the degree of the polymial, passed as the value of kpar)</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can use the function <b><code>cdist</code></b> as a helperfunction.</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bb5f61111872b189dd75463f121c2fe",
     "grade": false,
     "grade_id": "cell-948d761b9f7671fe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def computeK(kerneltype, X, Z, kpar=0):\n",
    "    \"\"\"\n",
    "    function K = computeK(kernel_type, X, Z)\n",
    "    computes a matrix K such that Kij=k(x,z);\n",
    "    for three different function linear, rbf or polynomial.\n",
    "    \n",
    "    Input:\n",
    "    kerneltype: either 'linear','polynomial','rbf'\n",
    "    X: n input vectors of dimension d (nxd);\n",
    "    Z: m input vectors of dimension d (mxd);\n",
    "    kpar: kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    OUTPUT:\n",
    "    K : nxm kernel matrix\n",
    "    \"\"\"\n",
    "    assert kerneltype in [\"linear\",\"polynomial\",\"poly\",\"rbf\"], \"Kernel type %s not known.\" % kerneltype\n",
    "    assert X.shape[1] == Z.shape[1], \"Input dimensions do not match\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f44d4844a8fa86381f7d845d2e9f5b8e",
     "grade": false,
     "grade_id": "cell-7ba9cde1b74a5ec6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Example Test</h3> (use this as a starting point for making new tests!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isClose(num1,num2,numtol=1e-3):\n",
    "    return np.abs(num1-num2)<numtol\n",
    "\n",
    "def isMatClose(X1,X2,tol=1e-3):\n",
    "    a,b = X1.shape\n",
    "    output = True\n",
    "    for i in range(a):\n",
    "        for j in range(b):\n",
    "            output = output and isClose(X1[i][j],X2[i][j],numtol=tol)\n",
    "    return output \n",
    "\n",
    "def testCase_computeK_linear():\n",
    "    X = np.array([[9],[0]])\n",
    "    Z = np.array([[1],[3]])\n",
    "    K = computeK('linear',X,Z)\n",
    "    K = np.array(K)\n",
    "    check = isMatClose(K,np.array([[9,27],[0,0]])) \n",
    "    return check\n",
    "\n",
    "print('computeK passes test for linear kernel: ' + str(testCase_computeK_linear()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ba548be87ec5092f1421cf419ebd0c6",
     "grade": true,
     "grade_id": "cell-caecc234e3612655",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 2: testCase_computeK_linear\n",
    "# ---------------------------------------\n",
    "# This tests whether the linear kernel is computed properly on an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09573efb7c32b4d015a42e848087460c",
     "grade": true,
     "grade_id": "cell-15181fab6eaee5a1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 3: testCase_computeK_polynomial\n",
    "# -------------------------------------------\n",
    "# This tests whether the polynomial kernel is computed properly on an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92d44736cd8bfac71b3bd4cb796e46e5",
     "grade": true,
     "grade_id": "cell-8cd4b6888cf1b058",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 4: testCase_computeK_rbf\n",
    "# ------------------------------------\n",
    "# This tests whether the rbf kernel is computed properly on an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ccf7ce40c01576ddcbbb800c93ff777",
     "grade": false,
     "grade_id": "cell-c1ebb4aee30b7b58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Additional Testing</h3>\n",
    "<p>The following code snippet plots an image of the kernel matrix for the data points in the spiral set. Use it to test your <b><code>computeK</code></b> function:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "K=computeK(\"rbf\",xTr,xTr,kpar=0.05)\n",
    "# plot an image of the kernel matrix\n",
    "fig = plt.figure()\n",
    "plt.pcolormesh(K, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fdf2679f995436cfa9a373b046002ee",
     "grade": false,
     "grade_id": "cell-6b72343891248ae1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Remember that the SVM optimization has the following dual formulation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "             &\\min_{\\alpha_1,\\cdots,\\alpha_n}\\frac{1}{2} \\sum_{i,j}\\alpha_i \\alpha_j y_i y_j \\mathbf{K}_{ij} - \\sum_{i=1}^{n}\\alpha_i  \\\\\n",
    "       \\text{s.t.}  &\\quad 0 \\leq \\alpha_i \\leq C\\\\\n",
    "             &\\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "This is equivalent to solving for the SVM primal\n",
    "$$ L(\\mathbf{w},b) = C\\sum_{i=1}^n \\max(1-y_i(\\mathbf{w}^\\top\\phi(\\mathbf{x}_i)+b),0) + ||w||_2^2$$\n",
    "where $\\mathbf{w}=\\sum_{i=1}^n y_i \\alpha_i \\phi(\\mathbf{x}_i)$ and $\\mathbf{K}_{ij}=k(\\mathbf{x}_i,\\mathbf{x}_j)=\\phi(\\mathbf{x}_i)^\\top\\phi(\\mathbf{x}_j)$, for some mapping $\\phi(\\cdot)$.  Please note that here all $\\alpha_i\\geq 0$, which is possible because we multiply by $y_i$ in the definition of $\\mathbf{w}$. One advantage of keeping all $\\alpha_i$ non-negative is that we can easily identify non-support vectors as vectors with $\\alpha_i=0$. \n",
    "\n",
    "<p>Implement the function <code>dualqp</code>, which takes as input a kernel matrix $K$, a vector of labels $yTr$ in $\\mathcal{R}^{n}$, and a regularization constant $C\\geq 0$. This function should solve the quadratic optimization problem and output the optimal vector $\\mathbf{\\alpha}\\in{\\mathcal{R}^n}$.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f6527c624dad2ea28e1e6fc1e65299e",
     "grade": false,
     "grade_id": "cell-a6fae38b01273eed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dualqp(K,yTr,C):\n",
    "    \"\"\"\n",
    "    function alpha = dualqp(K,yTr,C)\n",
    "    constructs the SVM dual formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        K     | the (nxn) kernel matrix\n",
    "        yTr   | training labels (nx1)\n",
    "        C     | the SVM regularization parameter\n",
    "    \n",
    "    Output:\n",
    "        alpha | the calculated solution vector (nx1)\n",
    "    \"\"\"\n",
    "    y = yTr.flatten()\n",
    "    N, _ = K.shape\n",
    "    alpha = Variable(N)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return np.array(alpha.value).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01f6bd91cad8c3480e650a660fa2279d",
     "grade": false,
     "grade_id": "cell-2ff9bc4ae9a92435",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following code shows a usecase of how <code>dualqp</code> could be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10\n",
    "lmbda = 0.25\n",
    "ktype = \"rbf\"\n",
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "# compute kernel (make sure it is PSD)\n",
    "K = computeK(ktype,xTr,xTr)\n",
    "eps = 1e-8\n",
    "# make sure it is symmetric and positive semi-definite\n",
    "K = (K + K.T) / 2 + eps * np.eye(K.shape[0])\n",
    "\n",
    "alpha=dualqp(K,yTr,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0aa2df4d10f8db8e37cedd2cba48bd4",
     "grade": false,
     "grade_id": "cell-ca3b1b5cd617938c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Testing Hint</h3> Create a dataset where you know what some of the optimal values of $\\alpha$ will be, and test to make sure that the solution gets those values of $\\alpha$ correct (recall from the lecture that the $\\alpha$ values associated with certain data points are guaranteed to have a specific optimal value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3be16a95293f0a3a24a16eb2413aa3b",
     "grade": true,
     "grade_id": "cell-c68ed2e610ca6492",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 5: testCase_dualQP\n",
    "# ------------------------------\n",
    "# This tests whether the alphas computed from dual qp are close to the correct values for an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f033081efa414b630afcb2661a92db44",
     "grade": false,
     "grade_id": "cell-547efb7fc9df6bb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p> Now that you can solve the dual correctly, you should have the values for $\\alpha_i$. But you are not done yet. You still need to be able to classify new test points. Remember from class that $h(\\mathbf{x})=\\sum_{i=1}^n \\alpha_i y_i k(\\mathbf{x}_i,\\mathbf{x})+b$. You need to obtain $b$. It is easy to show (and omitted here) that if $C>\\alpha_i>0$ (with strict $>$), then we must have that $y_i(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i)+b)=1$. Rephrase this equality in terms of $\\alpha_i$ and solve for $b$. Implement\n",
    "\n",
    "<p> b=recoverBias(K,yTr,alphas,C); </p>\n",
    "\n",
    "<p> where <code>b</code> is the hyperplane bias.\n",
    "(Hint: This is most stable if you pick an $\\alpha_i$ that is furthest from $C$ and $0$. )</p>\n",
    "\n",
    "<p>Please note that this use of the word bias has absolutely nothing to do with the word bias in the bias variance trade-off. It is just the same word but two completely different meanings. This unfortunate term collision comes from the fact that we are borrowing concepts from geometry and statistics.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "260d1f96ba24645b286c14b8ff763775",
     "grade": false,
     "grade_id": "cell-8d95dc58cb2c878b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def recoverBias(K,yTr,alpha,C):\n",
    "    \"\"\"\n",
    "    function bias=recoverBias(K,yTr,alpha,C);\n",
    "    Solves for the hyperplane bias term, which is uniquely specified by the \n",
    "    support vectors with alpha values 0<alpha<C\n",
    "    \n",
    "    INPUT:\n",
    "    K : nxn kernel matrix\n",
    "    yTr : nx1 input labels\n",
    "    alpha  : nx1 vector of alpha values\n",
    "    C : regularization constant\n",
    "    \n",
    "    Output:\n",
    "    bias : the scalar hyperplane bias of the kernel SVM specified by alphas\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7aa5daef68ca026fcdbda4bab14a8291",
     "grade": false,
     "grade_id": "cell-fabd039ad2be58bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Testing</h3>\n",
    "<p> Test your <b><code>recoverBias</code></b> function with the following code, which uses the dual solver on a linearly separable dataset:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr=genrandomdata(b=0.5)\n",
    "C=1\n",
    "K=computeK(\"linear\",xTr,xTr)\n",
    "eps=1e-8\n",
    "K = (K + K.T) / 2 + eps * np.eye(K.shape[0])\n",
    "alpha = dualqp(K,yTr,C)\n",
    "ba=recoverBias(K,yTr,alpha,C)\n",
    "wa = (alpha * yTr).dot(xTr)\n",
    "fun = lambda x: x.dot(wa) + ba\n",
    "visclassifier(fun, xTr, yTr, w=wa, b=ba)\n",
    "# Note: In some cases you may get an error or an odd plot. This is because sometimes \n",
    "#       the randomly generated data makes the kernel matrix close to singular. For \n",
    "#       graded hidden tests the randomness is fixed so this will not be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "414c1cfce8e6944d159252c1cee90540",
     "grade": true,
     "grade_id": "cell-1cc076276e9ddcd1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 6: testCase_recoverBias\n",
    "# -----------------------------------\n",
    "# This tests whether the bias computed from recoverBias is close to the correct value for an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44b1a6df6870e3eb92e2bedf9f0aaaaf",
     "grade": false,
     "grade_id": "cell-ff359b726e856377",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>\n",
    "    Implement the function \n",
    "    <pre>\n",
    "    svmclassify=dualSVM(xTr,yTr,C,ktype,kpar);\n",
    "    </pre>\n",
    "    It should use your functions <code><b>computeK</b></code> and <code><b>generateQP</b></code> to solve the SVM dual problem of an SVM specified by a training data set (<code><b>xTr,yTr</b></code>), a regularization parameter (<code>C</code>), a kernel type (<code>ktype</code>) and kernel parameter (<code>lmbda</code>, to be used as kpar in Kernel construction). Then, find the support vectors and recover the bias to return <b><code>svmclassify</code></b>, a function that uses your SVM to classify a set of test points <code>xTe</code>.\n",
    "\n",
    "<b>Hint: You need to ensure that the kernel matrix is positive semi-definite during training. The best way to do this is to make sure it is strictly symmetric and to add the identity matrix to it, multiplied by a tiny epsilon value.</b>\n",
    "    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ae77cb5d007cef05def4b752cb60e92",
     "grade": false,
     "grade_id": "cell-9b73c57876b214c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dualSVM(xTr,yTr,C,ktype,lmbda,eps=1e-8):\n",
    "    \"\"\"\n",
    "    function classifier = dualSVM(xTr,yTr,C,ktype,lmbda);\n",
    "    Constructs the SVM dual formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        xTr   | training data (nxd)\n",
    "        yTr   | training labels (nx1)\n",
    "        C     | the SVM regularization parameter\n",
    "        ktype | the type of kernelization: 'rbf','polynomial','linear'\n",
    "        lmbda | the kernel parameter - degree for poly, inverse width for rbf\n",
    "    \n",
    "    Output:\n",
    "        svmclassify | usage: predictions=svmclassify(xTe);\n",
    "    \"\"\"\n",
    "    svmclassify = lambda x: x # CHANGE THIS\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return svmclassify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b99751c280caf78eb91f27500d2b2f4",
     "grade": false,
     "grade_id": "cell-130d6e00d40a437f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Testing</h3>\n",
    "<p>Now we try the SVM with RBF kernel on the spiral data. <b>If you implemented it correctly, train and test error should be close to zero.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "C=10.0\n",
    "sigma=10.0 # Note: If sigma is too small you may get convergence errors.\n",
    "ktype=\"rbf\"\n",
    "svmclassify=dualSVM(xTr,yTr,C,ktype,sigma)\n",
    "\n",
    "visclassifier(svmclassify,xTr,yTr)\n",
    "\n",
    "# compute training and testing error\n",
    "predsTr=svmclassify(xTr)\n",
    "trainingerr=np.mean(np.sign(predsTr)!=yTr)\n",
    "print(\"Training error: %2.4f\" % trainingerr)\n",
    "\n",
    "predsTe=svmclassify(xTe)\n",
    "testingerr=np.mean(np.sign(predsTe)!=yTe)\n",
    "print(\"Testing error: %2.4f\" % testingerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88b00bd0da250b60223fa000f1282af3",
     "grade": true,
     "grade_id": "cell-bd5d833fe168a7a5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 7: testCase_dualSVM_easy_dataset\n",
    "# --------------------------------------------\n",
    "# This tests whether the function from dualSVM correctly classifies an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "368db643983b7ca23c756674fb7258a3",
     "grade": true,
     "grade_id": "cell-567ead8163bdad82",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 8: testCase_dualSVM_hard_dataset\n",
    "# --------------------------------------------\n",
    "# This tests whether the function from dualSVM correctly classifies a hard example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "965e50373b39856a5129f33ce920cc00",
     "grade": true,
     "grade_id": "cell-d6289896907469bc",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 9: testCase_dualSVM_hard_dataset2\n",
    "# ---------------------------------------------\n",
    "# This tests whether the function from dualSVM correctly classifies an even harder example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ce34cbba8c76366c22288c37325afe3",
     "grade": false,
     "grade_id": "cell-d482c8b79d383b69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "SVMs are pretty sensitive to hyper-parameters. We can visualize the results of a hyper-parameter grid search as a heat-map, where we sweep across different values of C and kpar and output the result on a validation dataset. Now we ask you to implement a cross-validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1634e788a41a0acf21ecdc7bde8b00f",
     "grade": false,
     "grade_id": "cell-523f3c6481d46ac1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList):\n",
    "    \"\"\"\n",
    "    function bestC,bestLmbda,ErrorMatrix = cross_validation(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList);\n",
    "    Use the parameter search to find the optimal parameter,\n",
    "    Individual models are trained on (xTr,yTr) while validated on (xValid,yValid)\n",
    "    \n",
    "    Input:\n",
    "        xTr      | training data (nxd)\n",
    "        yTr      | training labels (nx1)\n",
    "        xValid   | training data (mxd)\n",
    "        yValid   | training labels (mx1)\n",
    "        ktype    | the type of kernelization: 'rbf','polynomial','linear'\n",
    "        CList    | The list of values to try for the SVM regularization parameter C (ax1)\n",
    "        lmbdaList| The list of values to try for the kernel parameter lmbda- degree for poly, inverse width for rbf (bx1)\n",
    "    \n",
    "    Output:\n",
    "        bestC      | the best C parameter\n",
    "        bestLmbda  | the best Lmbda parameter\n",
    "        ErrorMatrix| the test error rate for each given C and Lmbda when trained on (xTr,yTr) and tested on (xValid,yValid),(axb)\n",
    "    \"\"\"\n",
    "    # gridsearch for best parameters\n",
    "    ErrorMatrix=np.zeros((len(CList),len(lmbdaList)))\n",
    "    bestC,bestLmbda = 0.,0.\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "            \n",
    "    return bestC,bestLmbda,ErrorMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29067c8be390014b6702de8684179a40",
     "grade": false,
     "grade_id": "cell-48b1ca185eae90d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(15)\n",
    "xTr,yTr,xValid,yValid=spiraldata(100)\n",
    "CList=(2.0**np.linspace(-2,1,7))\n",
    "lmbdaList=(np.linspace(1.0,15,7))\n",
    "\n",
    "bestC,bestLmbda,ErrorMatrix = cross_validation(xTr,yTr,xValid,yValid,'rbf',CList,lmbdaList)\n",
    "fig = plt.figure()\n",
    "plt.pcolormesh(ErrorMatrix, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"lmbda_idx\")\n",
    "plt.ylabel(\"C_idx\")\n",
    "plt.title(\"Validation error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1651c6b4a1deefa4ad928e328dc3f3d",
     "grade": false,
     "grade_id": "cell-7b2d2d652cab0f53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you implemented everything correctly, the result should look similar to this image:\n",
    "<center>\n",
    " <img src=\"crossval.png\" width=\"300px\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1006bba491c4b9fcf5eeb8e387301805",
     "grade": true,
     "grade_id": "cell-80f8696818cafe2c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 10: testCase_cv\n",
    "# ---------------------------\n",
    "# This tests whether the error matrix from cross_validation is correct for an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af44028c2efd027c5773b8ef6fe09752",
     "grade": false,
     "grade_id": "cell-8d9a00ede954a98e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Competition</h3>\n",
    "\n",
    "We ask you to implement the function autosvm(), which given xTr and yTr, splits them into training data and validation data, and then uses a hyperparameter search to find the optimal hyper parameters. \n",
    "\n",
    "Function autosvm should return a function which will act as a classifier on xTe.\n",
    "\n",
    "The competition will test multiple datasets, each dataset having different optimal hyperparameters, so you should strive for a good method of finding hyperparameters (within the time limit) instead of just trying to find a static set of good hyperparameters. \n",
    "\n",
    "You will get <b>5 points</b> for the competition if you can beat the base benchmark of <b>43%</b> error, and an additional <b>5 points</b> if you can get an error rate below <b>34%</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25fbafc4eb4609e5642cd017b0c1bf1e",
     "grade": false,
     "grade_id": "cell-0562da5629a8b501",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def autosvm(xTr,yTr):\n",
    "    \"\"\"\n",
    "    svmclassify = autosvm(xTr,yTr), where yTe = svmclassify(xTe)\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e00819254810d5fe2b00f4f7ed31f528",
     "grade": true,
     "grade_id": "cell-9db3d9cbb0582401",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 11: competition\n",
    "# ---------------------------\n",
    "# This tests the error rate of your classifier on the competition datasets \n",
    "# (remember each cell in this notebook should run in < 5 minutes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8561cb24065c7c8ef55483b5a06381c",
     "grade": true,
     "grade_id": "cell-4626183656c14d2e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Prints out the summary of tests passed/failed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
